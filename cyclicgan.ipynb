{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8053dedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign samples: 1200\n",
      "Malware class distribution:\n",
      " label\n",
      "4    109\n",
      "3    109\n",
      "2    101\n",
      "1     84\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Load your CSV\n",
    "df = pd.read_csv(\"./processed_data/merged_cic_andmal2017.csv\")\n",
    "\n",
    "# Map benign and malware labels\n",
    "benign_labels = [\"benign\", \"malware_2015_1016\"]\n",
    "df['label'] = df['label'].apply(lambda x: 0 if x in benign_labels else x)\n",
    "\n",
    "# Assign malware classes 1,2,3... based on unique labels\n",
    "malware_labels = df[df['label'] != 0]['label'].unique()\n",
    "malware_label_map = {label: idx + 1 for idx, label in enumerate(malware_labels)}\n",
    "df['label'] = df['label'].apply(lambda x: malware_label_map.get(x, x) if x != 0 else 0)\n",
    "\n",
    "# Undersample benign samples to 1200\n",
    "benign_df = df[df['label'] == 0]\n",
    "benign_sampled = resample(benign_df, replace=False, n_samples=1200, random_state=42)\n",
    "\n",
    "# Keep malware samples for augmentation\n",
    "malware_df = df[df['label'] != 0]\n",
    "\n",
    "print(\"Benign samples:\", len(benign_sampled))\n",
    "print(\"Malware class distribution:\\n\", malware_df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea50e1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator1D(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=12, out_features=64, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Generator for 1D tabular data\n",
    "class Generator1D(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Generator1D, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(64, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Discriminator for 1D tabular data\n",
    "class Discriminator1D(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator1D, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Dynamically determine input dimension based on the dataset\n",
    "input_dim = benign_sampled.select_dtypes(include=[np.number]).shape[1]\n",
    "\n",
    "# Initialize models with the correct input dimension\n",
    "G_AB = Generator1D(input_dim=input_dim)\n",
    "G_BA = Generator1D(input_dim=input_dim)\n",
    "D_A = Discriminator1D(input_dim=input_dim)\n",
    "D_B = Discriminator1D(input_dim=input_dim)\n",
    "\n",
    "# Move models to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "G_AB.to(device)\n",
    "G_BA.to(device)\n",
    "D_A.to(device)\n",
    "D_B.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40715b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders created with 6 batches of size 64\n",
      "Benign samples: 1200\n",
      "Malware samples: 403\n"
     ]
    }
   ],
   "source": [
    "# Replace the third cell with this improved implementation\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# Ensure only numeric columns are used\n",
    "benign_data = benign_sampled.select_dtypes(include=[np.number]).values\n",
    "malware_data = malware_df.select_dtypes(include=[np.number]).values\n",
    "\n",
    "# Get counts\n",
    "n_benign = len(benign_data)\n",
    "n_malware = len(malware_data)\n",
    "\n",
    "# Convert to tensors\n",
    "benign_tensor = torch.tensor(benign_data, dtype=torch.float32)\n",
    "malware_tensor = torch.tensor(malware_data, dtype=torch.float32)\n",
    "\n",
    "# Balance batch sizes - we'll use the smaller dataset size to determine batch structure\n",
    "batch_size = 64\n",
    "n_batches = min(n_benign, n_malware) // batch_size\n",
    "\n",
    "# Create separate dataloaders\n",
    "benign_dataset = TensorDataset(benign_tensor)\n",
    "malware_dataset = TensorDataset(malware_tensor)\n",
    "\n",
    "benign_loader = DataLoader(benign_dataset, batch_size=batch_size, shuffle=True)\n",
    "malware_loader = DataLoader(malware_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"DataLoaders created with {n_batches} batches of size {batch_size}\")\n",
    "print(f\"Benign samples: {n_benign}\")\n",
    "print(f\"Malware samples: {n_malware}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bd7db36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1000: Generator Loss nan, D_A Loss nan, D_B Loss nan\n",
      "Epoch 100/1000: Generator Loss nan, D_A Loss nan, D_B Loss nan\n",
      "Epoch 100/1000: Generator Loss nan, D_A Loss nan, D_B Loss nan\n",
      "Epoch 200/1000: Generator Loss nan, D_A Loss nan, D_B Loss nan\n",
      "Epoch 200/1000: Generator Loss nan, D_A Loss nan, D_B Loss nan\n",
      "Epoch 300/1000: Generator Loss nan, D_A Loss nan, D_B Loss nan\n",
      "Epoch 300/1000: Generator Loss nan, D_A Loss nan, D_B Loss nan\n",
      "Epoch 400/1000: Generator Loss nan, D_A Loss nan, D_B Loss nan\n",
      "Epoch 400/1000: Generator Loss nan, D_A Loss nan, D_B Loss nan\n",
      "Epoch 500/1000: Generator Loss nan, D_A Loss nan, D_B Loss nan\n",
      "Epoch 500/1000: Generator Loss nan, D_A Loss nan, D_B Loss nan\n",
      "Epoch 600/1000: Generator Loss nan, D_A Loss nan, D_B Loss nan\n",
      "Epoch 600/1000: Generator Loss nan, D_A Loss nan, D_B Loss nan\n",
      "Epoch 700/1000: Generator Loss nan, D_A Loss nan, D_B Loss nan\n",
      "Epoch 700/1000: Generator Loss nan, D_A Loss nan, D_B Loss nan\n",
      "Epoch 800/1000: Generator Loss nan, D_A Loss nan, D_B Loss nan\n",
      "Epoch 800/1000: Generator Loss nan, D_A Loss nan, D_B Loss nan\n",
      "Epoch 900/1000: Generator Loss nan, D_A Loss nan, D_B Loss nan\n",
      "Epoch 900/1000: Generator Loss nan, D_A Loss nan, D_B Loss nan\n"
     ]
    }
   ],
   "source": [
    "# Replace the training loop cell with this fixed version\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# Loss functions\n",
    "adversarial_loss = nn.MSELoss()\n",
    "cycle_loss = nn.L1Loss()\n",
    "\n",
    "# Optimizers\n",
    "lr = 0.0002\n",
    "optimizer_G = optim.Adam(list(G_AB.parameters()) + list(G_BA.parameters()), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D_A = optim.Adam(D_A.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "optimizer_D_B = optim.Adam(D_B.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "loss_history_G = []\n",
    "loss_history_D_A = []\n",
    "loss_history_D_B = []\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss_G = 0\n",
    "    epoch_loss_D_A = 0\n",
    "    epoch_loss_D_B = 0\n",
    "    batches = 0\n",
    "    \n",
    "    # Create iterators for the dataloaders\n",
    "    benign_iter = iter(benign_loader)\n",
    "    malware_iter = iter(malware_loader)\n",
    "    \n",
    "    for i in range(min(len(benign_loader), len(malware_loader))):\n",
    "        try:\n",
    "            real_A = next(benign_iter)[0].to(device)\n",
    "            real_B = next(malware_iter)[0].to(device)\n",
    "            \n",
    "            # Ensure batch sizes match by truncating the larger batch\n",
    "            min_batch_size = min(real_A.size(0), real_B.size(0))\n",
    "            real_A = real_A[:min_batch_size]\n",
    "            real_B = real_B[:min_batch_size]\n",
    "\n",
    "            # Dynamically create valid and fake tensors to match the batch size\n",
    "            valid = torch.ones(min_batch_size, 1).to(device)\n",
    "            fake = torch.zeros(min_batch_size, 1).to(device)\n",
    "\n",
    "            # ------------------\n",
    "            #  Train Generators\n",
    "            # ------------------\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            fake_B = G_AB(real_A)\n",
    "            recov_A = G_BA(fake_B)\n",
    "\n",
    "            fake_A = G_BA(real_B)\n",
    "            recov_B = G_AB(fake_A)\n",
    "\n",
    "            loss_GAN_AB = adversarial_loss(D_B(fake_B), valid)\n",
    "            loss_GAN_BA = adversarial_loss(D_A(fake_A), valid)\n",
    "            loss_cycle_A = cycle_loss(recov_A, real_A)\n",
    "            loss_cycle_B = cycle_loss(recov_B, real_B)\n",
    "\n",
    "            loss_G = loss_GAN_AB + loss_GAN_BA + 10 * (loss_cycle_A + loss_cycle_B)\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # -----------------------\n",
    "            #  Train Discriminator A\n",
    "            # -----------------------\n",
    "            optimizer_D_A.zero_grad()\n",
    "            loss_real = adversarial_loss(D_A(real_A), valid)\n",
    "            loss_fake = adversarial_loss(D_A(fake_A.detach()), fake)\n",
    "            loss_D_A = (loss_real + loss_fake) * 0.5\n",
    "            loss_D_A.backward()\n",
    "            optimizer_D_A.step()\n",
    "\n",
    "            # -----------------------\n",
    "            #  Train Discriminator B\n",
    "            # -----------------------\n",
    "            optimizer_D_B.zero_grad()\n",
    "            loss_real = adversarial_loss(D_B(real_B), valid)\n",
    "            loss_fake = adversarial_loss(D_B(fake_B.detach()), fake)\n",
    "            loss_D_B = (loss_real + loss_fake) * 0.5\n",
    "            loss_D_B.backward()\n",
    "            optimizer_D_B.step()\n",
    "            \n",
    "            # Track losses\n",
    "            epoch_loss_G += loss_G.item()\n",
    "            epoch_loss_D_A += loss_D_A.item()\n",
    "            epoch_loss_D_B += loss_D_B.item()\n",
    "            batches += 1\n",
    "            \n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "    # Store average losses for this epoch\n",
    "    loss_history_G.append(epoch_loss_G / batches)\n",
    "    loss_history_D_A.append(epoch_loss_D_A / batches)\n",
    "    loss_history_D_B.append(epoch_loss_D_B / batches)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}/{num_epochs}: Generator Loss {loss_history_G[-1]:.4f}, D_A Loss {loss_history_D_A[-1]:.4f}, D_B Loss {loss_history_D_B[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc165e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Generator and Discriminator Losses\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss_history_G, label='Generator Loss', linewidth=2)\n",
    "plt.plot(loss_history_D_A, label='Discriminator A Loss', linestyle='--')\n",
    "plt.plot(loss_history_D_B, label='Discriminator B Loss', linestyle='--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('CycleGAN Training Loss Over Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c667040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the sample generation and final dataset creation cells\n",
    "\n",
    "# Improved sample generation function\n",
    "def generate_samples(generator, base_samples, target_count, batch_size=64):\n",
    "    generated = []\n",
    "    \n",
    "    # Process in batches to prevent memory issues\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(base_samples), batch_size):\n",
    "            batch = base_samples[i:i+batch_size]\n",
    "            base_tensor = torch.tensor(batch, dtype=torch.float32).to(device)\n",
    "            fake_data = generator(base_tensor).cpu().numpy()\n",
    "            generated.extend(fake_data)\n",
    "    \n",
    "    # If we need more samples than we generated, repeat the process\n",
    "    while len(generated) < target_count:\n",
    "        # Randomly select from what we've already generated\n",
    "        idx = np.random.choice(len(generated), min(batch_size, target_count - len(generated)))\n",
    "        batch = np.array([generated[i] for i in idx])\n",
    "        base_tensor = torch.tensor(batch, dtype=torch.float32).to(device)\n",
    "        fake_data = generator(base_tensor).cpu().numpy()\n",
    "        generated.extend(fake_data)\n",
    "    \n",
    "    return np.array(generated[:target_count])\n",
    "\n",
    "# Balance the dataset - set target count per class\n",
    "target_per_class = 1200  # Same as benign samples\n",
    "balanced_data = []\n",
    "balanced_labels = []\n",
    "\n",
    "# Add all benign samples\n",
    "balanced_data.append(benign_sampled.drop(columns=['label']).values)\n",
    "balanced_labels.extend([0] * len(benign_sampled))\n",
    "\n",
    "# Get unique malware classes\n",
    "malware_classes = sorted(malware_df['label'].unique())\n",
    "\n",
    "for malware_class in malware_classes:\n",
    "    # Get real samples for this class\n",
    "    class_samples = malware_df[malware_df['label'] == malware_class].drop(columns=['label']).values\n",
    "    class_count = len(class_samples)\n",
    "    \n",
    "    print(f\"Processing malware class {malware_class}: {class_count} real samples available\")\n",
    "    \n",
    "    if class_count >= target_per_class:\n",
    "        # Downsample if we have too many\n",
    "        selected_idx = np.random.choice(class_count, target_per_class, replace=False)\n",
    "        balanced_data.append(class_samples[selected_idx])\n",
    "    else:\n",
    "        # Use all real samples\n",
    "        balanced_data.append(class_samples)\n",
    "        \n",
    "        # Generate synthetic samples to reach the target\n",
    "        needed = target_per_class - class_count\n",
    "        print(f\"  Generating {needed} synthetic samples\")\n",
    "        synthetic_samples = generate_samples(G_AB, class_samples, needed)\n",
    "        balanced_data.append(synthetic_samples)\n",
    "    \n",
    "    # Add labels\n",
    "    balanced_labels.extend([malware_class] * target_per_class)\n",
    "\n",
    "# Combine all data\n",
    "all_data = np.vstack(balanced_data)\n",
    "final_df = pd.DataFrame(all_data, columns=benign_sampled.columns.drop('label'))\n",
    "final_df['label'] = balanced_labels\n",
    "\n",
    "# Save balanced dataset\n",
    "final_df.to_csv(\"balanced_augmented_dataset.csv\", index=False)\n",
    "print(f\"Final balanced dataset created with {len(final_df)} samples\")\n",
    "print(\"Class distribution:\")\n",
    "print(final_df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e23b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After generating synthetic samples for each malware class\n",
    "final_dataset = pd.concat([benign_sampled, malware_df], ignore_index=True)\n",
    "\n",
    "# Replace original data with generated malware samples as needed\n",
    "final_dataset.to_csv(\"balanced_augmented_dataset.csv\", index=False)\n",
    "print(\"Final dataset saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "provenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
